1) 체크포인트 디렉토리 안에 저장

checkpoint_filepath = './ckpt/'
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=False,
    monitor='val_loss',
    mode='auto',
    save_best_only=True)

위와 같이 저장할 경우 후에 복원시 아래와 같이 호출하면 된다.
- (functional API에서만 동작)
- (subclass일 경우 save_weights, load_weights를 써야함 -> h5, pb파일이 save해서는 안만들어짐)
model = tf.keras.models.load_model('./ckpt/')
for x in model.trainable_variables:
    print(x.name)

- (tf.keras.model의 Subclass인 경우. tf 2.2.0에서 잘 동작. 2.1, 2.3에서 버그가 있다고 한다.)
- (ModelCheckpoint를 통해서 save_weights_only=False하긴 힘들다. custom callback을 이용하자!)
https://stackoverflow.com/questions/51806852/cant-save-custom-subclassed-model
# ckpt란 폴더에 저장하게 된다...모델 전체를. 따라서 불러오기만 하면 됨.
gs.best_estimator_.model.save('ckpt')
mm = tf.keras.models.load_model('ckpt')
! fit 안하고 저장하면 input_shape어쩌구 에러나니 주의!



save_weights_only=True 로 한 경우
model = tf.keras.Model()
model을 층, 각 층별 이름까지(순서 같으면 동일한 디폴트이름) 동일하게 설정한 후 
model.load_weights('./ckpt') 하면 된다

2) 과적합 여부 조사 위한 train/val loss 그래프
history = model.fit(x, y, 
          epochs=20, 
          batch_size=32,
          validation_data=(test_data, test_data),
          shuffle=True)

plt.plot(history.history["loss"], label="Training Loss")
plt.plot(history.history["val_loss"], label="Validation Loss")
plt.legend()

val_ 값들은 model에 compile 해준 대로...

3) custom callbacks
https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback
위 링크의 callback을 상속하여 각 메소드를 원하는 대로 구현
on_epoch_end(epoch, logs) 가 가장 많이 쓰일 듯


4) custom train 
https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit
클래스로 상속했다면 train_step, compiled_loss, compiled_metrics.update_state 를 잘 사용하여 정의
# loss계산
 with tf.GradientTape() as tape:
            y_pred = self(x, training=True)  # Forward pass
            # 여기서 loss는 추후 model.compile()에서 정의한 것
            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)

# Compute gradients. (opt에 적용하는 것이 전파 일으킴)
    trainable_vars = self.trainable_variables
    gradients = tape.gradient(loss, trainable_vars)
    # Update weights
    self.optimizer.apply_gradients(zip(gradients, trainable_vars))


5) custom loss
tf.keras.losses 를 상속하여 정의하여도 되고
compute_loss(y_true, y_pred) 형태로 정의한 후 compile시 loss에 넘겨주어도 된다